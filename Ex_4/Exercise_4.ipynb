{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Recognization and Machine Learning\n",
    "#### Exercise 4\n",
    "### Task 3\n",
    "#### Implement gradient descent for log-loss.\n",
    "\n",
    "a) Implement a log-loss minimization algorithm for the loss of Equation.\n",
    "\n",
    "b) Apply the code for the data log_loss_data X.scv and Y.csv. The data is in CSV format. Load X and y using numpy.loadtxt.\n",
    "\n",
    "c) Plot the path of w over 100 iterations and check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: w = [-1.61077699  6.98804232] (log-loss = 6400.49)\n",
      "Iteration 1: w = [-1.30784233  6.78337571] (log-loss = 6149.70)\n",
      "Iteration 2: w = [-1.01807052  6.57630451] (log-loss = 5900.48)\n",
      "Iteration 3: w = [-0.74104888  6.36752118] (log-loss = 5653.27)\n",
      "Iteration 4: w = [-0.47645165  6.15776289] (log-loss = 5408.61)\n",
      "Iteration 5: w = [-0.22424352  5.94783477] (log-loss = 5167.17)\n",
      "Iteration 6: w = [0.01507978 5.73864477] (log-loss = 4929.87)\n",
      "Iteration 7: w = [0.24028557 5.53124896] (log-loss = 4697.92)\n",
      "Iteration 8: w = [0.44931875 5.32688626] (log-loss = 4472.91)\n",
      "Iteration 9: w = [0.63947123 5.12696068] (log-loss = 4256.72)\n",
      "Iteration 10: w = [0.80792106 4.93293232] (log-loss = 4051.25)\n",
      "Iteration 11: w = [0.9525058  4.74613814] (log-loss = 3858.07)\n",
      "Iteration 12: w = [1.07227464 4.56764329] (log-loss = 3678.22)\n",
      "Iteration 13: w = [1.16750141 4.39819999] (log-loss = 3512.14)\n",
      "Iteration 14: w = [1.23935522 4.23827521] (log-loss = 3359.81)\n",
      "Iteration 15: w = [1.28961454 4.08806051] (log-loss = 3220.88)\n",
      "Iteration 16: w = [1.32056219 3.94744138] (log-loss = 3094.61)\n",
      "Iteration 17: w = [1.33491604 3.81598905] (log-loss = 2979.96)\n",
      "Iteration 18: w = [1.33563226 3.69303725] (log-loss = 2875.62)\n",
      "Iteration 19: w = [1.32561739 3.57781932] (log-loss = 2780.26)\n",
      "Iteration 20: w = [1.30749165 3.4695945 ] (log-loss = 2692.66)\n",
      "Iteration 21: w = [1.28347387 3.36772315] (log-loss = 2611.77)\n",
      "Iteration 22: w = [1.25536748 3.27169341] (log-loss = 2536.74)\n",
      "Iteration 23: w = [1.22460104 3.18111755] (log-loss = 2466.95)\n",
      "Iteration 24: w = [1.192288   3.09571368] (log-loss = 2401.89)\n",
      "Iteration 25: w = [1.15928705 3.01528246] (log-loss = 2341.22)\n",
      "Iteration 26: w = [1.12625549 2.93968356] (log-loss = 2284.66)\n",
      "Iteration 27: w = [1.09369331 2.8688141 ] (log-loss = 2232.02)\n",
      "Iteration 28: w = [1.06197829 2.80259023] (log-loss = 2183.15)\n",
      "Iteration 29: w = [1.03139317 2.74093195] (log-loss = 2137.91)\n",
      "Iteration 30: w = [1.00214624 2.68375148] (log-loss = 2096.17)\n",
      "Iteration 31: w = [0.97438669 2.63094498] (log-loss = 2057.82)\n",
      "Iteration 32: w = [0.94821603 2.58238744] (log-loss = 2022.72)\n",
      "Iteration 33: w = [0.92369652 2.53793049] (log-loss = 1990.72)\n",
      "Iteration 34: w = [0.90085764 2.49740252] (log-loss = 1961.68)\n",
      "Iteration 35: w = [0.87970109 2.4606109 ] (log-loss = 1935.42)\n",
      "Iteration 36: w = [0.86020512 2.42734555] (log-loss = 1911.78)\n",
      "Iteration 37: w = [0.84232827 2.39738335] (log-loss = 1890.56)\n",
      "Iteration 38: w = [0.82601285 2.37049306] (log-loss = 1871.59)\n",
      "Iteration 39: w = [0.81118825 2.34644008] (log-loss = 1854.68)\n",
      "Iteration 40: w = [0.7977741  2.32499102] (log-loss = 1839.65)\n",
      "Iteration 41: w = [0.78568317 2.3059175 ] (log-loss = 1826.33)\n",
      "Iteration 42: w = [0.77482406 2.28899947] (log-loss = 1814.54)\n",
      "Iteration 43: w = [0.76510358 2.27402761] (log-loss = 1804.14)\n",
      "Iteration 44: w = [0.75642878 2.26080517] (log-loss = 1794.98)\n",
      "Iteration 45: w = [0.74870865 2.24914902] (log-loss = 1786.92)\n",
      "Iteration 46: w = [0.74185542 2.23889026] (log-loss = 1779.84)\n",
      "Iteration 47: w = [0.73578561 2.22987429] (log-loss = 1773.63)\n",
      "Iteration 48: w = [0.73042069 2.22196054] (log-loss = 1768.19)\n",
      "Iteration 49: w = [0.7256875  2.21502199] (log-loss = 1763.43)\n",
      "Iteration 50: w = [0.72151853 2.20894439] (log-loss = 1759.27)\n",
      "Iteration 51: w = [0.7178519  2.20362545] (log-loss = 1755.62)\n",
      "Iteration 52: w = [0.71463128 2.19897395] (log-loss = 1752.44)\n",
      "Iteration 53: w = [0.71180568 2.19490878] (log-loss = 1749.67)\n",
      "Iteration 54: w = [0.70932917 2.19135807] (log-loss = 1747.24)\n",
      "Iteration 55: w = [0.70716059 2.18825824] (log-loss = 1745.13)\n",
      "Iteration 56: w = [0.70526315 2.18555322] (log-loss = 1743.29)\n",
      "Iteration 57: w = [0.70360411 2.18319361] (log-loss = 1741.68)\n",
      "Iteration 58: w = [0.70215442 2.18113598] (log-loss = 1740.28)\n",
      "Iteration 59: w = [0.70088833 2.1793422 ] (log-loss = 1739.06)\n",
      "Iteration 60: w = [0.69978312 2.17777883] (log-loss = 1737.99)\n",
      "Iteration 61: w = [0.69881874 2.17641656] (log-loss = 1737.07)\n",
      "Iteration 62: w = [0.69797755 2.17522975] (log-loss = 1736.26)\n",
      "Iteration 63: w = [0.69724405 2.17419598] (log-loss = 1735.56)\n",
      "Iteration 64: w = [0.69660463 2.17329563] (log-loss = 1734.95)\n",
      "Iteration 65: w = [0.69604736 2.17251158] (log-loss = 1734.41)\n",
      "Iteration 66: w = [0.69556178 2.17182889] (log-loss = 1733.95)\n",
      "Iteration 67: w = [0.69513875 2.17123451] (log-loss = 1733.55)\n",
      "Iteration 68: w = [0.69477028 2.17071705] (log-loss = 1733.20)\n",
      "Iteration 69: w = [0.69444936 2.1702666 ] (log-loss = 1732.89)\n",
      "Iteration 70: w = [0.6941699 2.1698745] (log-loss = 1732.62)\n",
      "Iteration 71: w = [0.69392657 2.16953321] (log-loss = 1732.39)\n",
      "Iteration 72: w = [0.69371472 2.16923617] (log-loss = 1732.19)\n",
      "Iteration 73: w = [0.69353029 2.16897764] (log-loss = 1732.01)\n",
      "Iteration 74: w = [0.69336974 2.16875263] (log-loss = 1731.86)\n",
      "Iteration 75: w = [0.69322998 2.16855682] (log-loss = 1731.73)\n",
      "Iteration 76: w = [0.69310834 2.16838641] (log-loss = 1731.61)\n",
      "Iteration 77: w = [0.69300247 2.16823812] (log-loss = 1731.51)\n",
      "Iteration 78: w = [0.69291032 2.16810907] (log-loss = 1731.43)\n",
      "Iteration 79: w = [0.69283013 2.16799677] (log-loss = 1731.35)\n",
      "Iteration 80: w = [0.69276034 2.16789905] (log-loss = 1731.28)\n",
      "Iteration 81: w = [0.6926996  2.16781402] (log-loss = 1731.23)\n",
      "Iteration 82: w = [0.69264675 2.16774002] (log-loss = 1731.18)\n",
      "Iteration 83: w = [0.69260075 2.16767564] (log-loss = 1731.13)\n",
      "Iteration 84: w = [0.69256073 2.16761961] (log-loss = 1731.09)\n",
      "Iteration 85: w = [0.6925259  2.16757086] (log-loss = 1731.06)\n",
      "Iteration 86: w = [0.69249559 2.16752844] (log-loss = 1731.03)\n",
      "Iteration 87: w = [0.69246922 2.16749153] (log-loss = 1731.01)\n",
      "Iteration 88: w = [0.69244627 2.16745941] (log-loss = 1730.98)\n",
      "Iteration 89: w = [0.6924263  2.16743147] (log-loss = 1730.97)\n",
      "Iteration 90: w = [0.69240892 2.16740715] (log-loss = 1730.95)\n",
      "Iteration 91: w = [0.69239381 2.167386  ] (log-loss = 1730.94)\n",
      "Iteration 92: w = [0.69238065 2.16736759] (log-loss = 1730.92)\n",
      "Iteration 93: w = [0.6923692  2.16735157] (log-loss = 1730.91)\n",
      "Iteration 94: w = [0.69235924 2.16733763] (log-loss = 1730.90)\n",
      "Iteration 95: w = [0.69235058 2.1673255 ] (log-loss = 1730.89)\n",
      "Iteration 96: w = [0.69234304 2.16731495] (log-loss = 1730.89)\n",
      "Iteration 97: w = [0.69233647 2.16730577] (log-loss = 1730.88)\n",
      "Iteration 98: w = [0.69233076 2.16729778] (log-loss = 1730.88)\n",
      "Iteration 99: w = [0.6923258  2.16729083] (log-loss = 1730.87)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def log_loss(w, X, y):\n",
    "    \"\"\" \n",
    "    Computes the log-loss function at w. The \n",
    "    computation uses the data in X with\n",
    "    corresponding labels in y. \n",
    "    \"\"\"\n",
    "    \n",
    "    L = 0 # Accumulate loss terms here.\n",
    "     \n",
    "        \n",
    "    # Process each sample in X:\n",
    "    for n in range(X.shape[0]):\n",
    "        L += np.log(1 + np.exp(y[n] * np.dot(w, X[n])))\n",
    "    \n",
    "    return L\n",
    "    \n",
    "def grad(w, X, y):\n",
    "    \"\"\" \n",
    "    Computes the gradient of the log-loss function\n",
    "    at w. The computation uses the data in X with\n",
    "    corresponding labels in y. \n",
    "    \"\"\"\n",
    "        \n",
    "    G = 0 # Accumulate gradient here.\n",
    "    \n",
    "    # Process each sample in X:\n",
    "    for n in range(X.shape[0]):\n",
    "        \n",
    "        numerator =  np.exp(-y[n] * np.dot(w, X[n])) * (-y[n])*X[n]\n",
    "        denominator = 1 + np.exp(-y[n] * np.dot(w, X[n]))\n",
    "        \n",
    "        G += numerator / denominator\n",
    "    \n",
    "    return G\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    # Add your code here:\n",
    "        \n",
    "    # 1) Load X and y.\n",
    "    X=np.loadtxt('X.csv',delimiter=\",\")\n",
    "    y=np.loadtxt('y.csv')\n",
    "\n",
    "    # 2) Initialize w at w = np.array([1, -1])\n",
    "    w = np.array([1, -1])\n",
    "    \n",
    "    # 3) Set step_size to a small positive value.\n",
    "    step_size=0.01\n",
    "\n",
    "    # 4) Initialize empty lists for storing the path and\n",
    "    # accuracies: W = []; accuracies = []\n",
    "    W = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for iteration in range(100):\n",
    "\n",
    "        # 5) Apply the gradient descent rule.\n",
    "        G = grad(w,X,y)\n",
    "        w = w - step_size*G\n",
    "        \n",
    "\n",
    "        # 6) Print the current state.\n",
    "        print (\"Iteration %d: w = %s (log-loss = %.2f)\" % \\\n",
    "              (iteration, str(w), log_loss(w, X, y)))\n",
    "        \n",
    "        # 7) Compute the accuracy (already done for you)\n",
    "            \n",
    "        # Predict class 1 probability\n",
    "        y_prob = 1 / (1 + np.exp(-np.dot(X, w)))\n",
    "                # Threshold at 0.5 (results are 0 and 1)\n",
    "        y_pred = (y_prob > 0.5).astype(int)\n",
    "                # Transform [0,1] coding to [-1,1] coding\n",
    "        y_pred = 2*y_pred - 1\n",
    "\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        W.append(w)\n",
    "    \n",
    "    # 8) Below is a template for plotting. Feel free to \n",
    "    # rewrite if you prefer different style.\n",
    "    \n",
    "    W = np.array(W)\n",
    "    \n",
    "    plt.figure(figsize = [5,5])\n",
    "    plt.subplot(211)\n",
    "    plt.plot(W[:,0], W[:,1], 'ro-')\n",
    "    plt.xlabel('w$_0$')\n",
    "    plt.ylabel('w$_1$')\n",
    "    plt.title('Optimization path')\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.plot(100.0 * np.array(accuracies), linewidth = 2)\n",
    "    plt.ylabel('Accuracy / %')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"log_loss_minimization.pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "#### Define the network in Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               51300     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 77,966\n",
      "Trainable params: 77,966\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "model= Sequential()\n",
    "\n",
    "N = 32 # Number of feature maps\n",
    "w, h = 5, 5 # Conv. window size\n",
    "model.add(Conv2D(N, (w, h),input_shape=(64, 64, 1),activation = 'relu',padding = 'same')) \n",
    "# rectified linear unit-Activation function\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "model.add(Conv2D(N, (w, h),activation = 'relu',padding = 'same')) \n",
    "model.add(MaxPooling2D((4,4)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "#### Compile and train the net.\n",
    "\n",
    "a) Compile the network following the examples of the lecture slides and documentation at http://keras.io/.\n",
    "\n",
    "b) Train the model with the GTSRB dataset from last week.\n",
    "\n",
    "Use the following parameters:\n",
    "\n",
    "Loss: categorical crossentropy (same thing as log loss; see previous\n",
    "exercises)\n",
    "\n",
    "Optimizer: stochastic gradient descent\n",
    "\n",
    "Minibatch size: 32\n",
    "\n",
    "Number of epochs: 20\n",
    "\n",
    "Also add the parameter metrics=[’accuracy’] as an argument of model.compile and give the test data to training algorithm.\n",
    "\n",
    "model.fit(..., validation_data = [X_test, y_test]). \n",
    "\n",
    "Then, the optimizer will report the test error every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from simplelbp import local_binary_pattern\n",
    "\n",
    "def load_data(folder):\n",
    "    \"\"\" \n",
    "    Load all images from subdirectories of\n",
    "    'folder'. The subdirectory name indicates\n",
    "    the class.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []          # Images go here\n",
    "    y = []          # Class labels go here\n",
    "    classes = []    # All class names go here\n",
    "    \n",
    "    subdirectories = glob.glob(folder + \"/*\")\n",
    "    \n",
    "    # Loop over all folders\n",
    "    for d in subdirectories:\n",
    "        \n",
    "        # Find all files from this folder\n",
    "        files = glob.glob(d + os.sep + \"*.jpg\")\n",
    "        \n",
    "        # Load all files\n",
    "        for name in files:\n",
    "            \n",
    "            # Load image and parse class name\n",
    "            img = plt.imread(name)\n",
    "            class_name = name.split(os.sep)[-2]\n",
    "\n",
    "            # Convert class names to integer indices:\n",
    "            if class_name not in classes:\n",
    "                classes.append(class_name)\n",
    "            \n",
    "            class_idx = classes.index(class_name)\n",
    "            \n",
    "            X.append(img)\n",
    "            y.append(class_idx)\n",
    "    \n",
    "    # Convert python lists to contiguous numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y\n",
    "X, y = load_data(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train,  y_test = train_test_split(X, y, test_size=0.20, random_state=50)\n",
    "\n",
    "# Keras assumes 4D input, but MNIST is lacking color channel.\n",
    "# -> Add a dummy dimension at the end.\n",
    "X_train = X_train[..., np.newaxis] / 255.0\n",
    "X_test = X_test[..., np.newaxis] / 255.0\n",
    "# Output has to be one-hot-encoded\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 161 samples, validate on 41 samples\n",
      "Epoch 1/20\n",
      "161/161 [==============================] - 6s 35ms/sample - loss: 0.6782 - accuracy: 0.5155 - val_loss: 0.6785 - val_accuracy: 0.5854\n",
      "Epoch 2/20\n",
      "161/161 [==============================] - 2s 14ms/sample - loss: 0.6707 - accuracy: 0.5342 - val_loss: 0.6757 - val_accuracy: 0.4878\n",
      "Epoch 3/20\n",
      "161/161 [==============================] - 2s 10ms/sample - loss: 0.6619 - accuracy: 0.5031 - val_loss: 0.6709 - val_accuracy: 0.5610\n",
      "Epoch 4/20\n",
      "161/161 [==============================] - 2s 10ms/sample - loss: 0.6571 - accuracy: 0.5466 - val_loss: 0.6716 - val_accuracy: 0.5122\n",
      "Epoch 5/20\n",
      "161/161 [==============================] - 2s 10ms/sample - loss: 0.6505 - accuracy: 0.5155 - val_loss: 0.6668 - val_accuracy: 0.7561\n",
      "Epoch 6/20\n",
      "161/161 [==============================] - 2s 10ms/sample - loss: 0.6564 - accuracy: 0.7888 - val_loss: 0.6603 - val_accuracy: 0.5366\n",
      "Epoch 7/20\n",
      "161/161 [==============================] - 2s 10ms/sample - loss: 0.6434 - accuracy: 0.5901 - val_loss: 0.6643 - val_accuracy: 0.8049\n",
      "Epoch 8/20\n",
      "161/161 [==============================] - 2s 9ms/sample - loss: 0.6530 - accuracy: 0.8385 - val_loss: 0.6565 - val_accuracy: 0.7805\n",
      "Epoch 9/20\n",
      "161/161 [==============================] - 2s 10ms/sample - loss: 0.6431 - accuracy: 0.8385 - val_loss: 0.6489 - val_accuracy: 0.6585\n",
      "Epoch 10/20\n",
      "161/161 [==============================] - 2s 10ms/sample - loss: 0.6299 - accuracy: 0.7019 - val_loss: 0.6511 - val_accuracy: 0.5366\n",
      "Epoch 11/20\n",
      "161/161 [==============================] - 2s 10ms/sample - loss: 0.6236 - accuracy: 0.5776 - val_loss: 0.6359 - val_accuracy: 0.7317\n",
      "Epoch 12/20\n",
      "161/161 [==============================] - 2s 11ms/sample - loss: 0.6150 - accuracy: 0.7640 - val_loss: 0.6444 - val_accuracy: 0.5610\n",
      "Epoch 13/20\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.6121 - accuracy: 0.64 - 2s 10ms/sample - loss: 0.6120 - accuracy: 0.6460 - val_loss: 0.6296 - val_accuracy: 0.7317\n",
      "Epoch 14/20\n",
      "161/161 [==============================] - 2s 9ms/sample - loss: 0.6020 - accuracy: 0.7391 - val_loss: 0.6303 - val_accuracy: 0.7805\n",
      "Epoch 15/20\n",
      "161/161 [==============================] - 2s 9ms/sample - loss: 0.6065 - accuracy: 0.8323 - val_loss: 0.6233 - val_accuracy: 0.7805\n",
      "Epoch 16/20\n",
      "161/161 [==============================] - 1s 9ms/sample - loss: 0.6035 - accuracy: 0.8261 - val_loss: 0.6207 - val_accuracy: 0.7073\n",
      "Epoch 17/20\n",
      "161/161 [==============================] - 2s 10ms/sample - loss: 0.5885 - accuracy: 0.7950 - val_loss: 0.6300 - val_accuracy: 0.6585\n",
      "Epoch 18/20\n",
      "161/161 [==============================] - 1s 9ms/sample - loss: 0.5849 - accuracy: 0.7764 - val_loss: 0.6394 - val_accuracy: 0.6098\n",
      "Epoch 19/20\n",
      "161/161 [==============================] - 2s 9ms/sample - loss: 0.5839 - accuracy: 0.7516 - val_loss: 0.5962 - val_accuracy: 0.7561\n",
      "Epoch 20/20\n",
      "161/161 [==============================] - 2s 10ms/sample - loss: 0.5659 - accuracy: 0.8199 - val_loss: 0.6158 - val_accuracy: 0.7073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26b0bd9c808>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='SGD',metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 20, batch_size=32, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
